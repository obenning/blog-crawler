# Blog Content Crawler fÃ¼r Claude AI

Dieses Repository crawlt automatisch alle Blog-Artikel von deiner Squarespace-Website und speichert sie als strukturierte Markdown-Dateien. Die Inhalte kÃ¶nnen dann direkt an Claude AI als Wissensgrundlage Ã¼bergeben werden.

## ğŸ¯ Zweck

- **Automatisches Crawling:** Alle Blog-Artikel werden regelmÃ¤ÃŸig gecrawlt
- **Strukturierte Speicherung:** Artikel werden als Markdown mit Metadaten gespeichert
- **Claude-kompatibel:** Inhalte sind optimiert fÃ¼r die Nutzung mit Claude AI
- **Versionskontrolle:** Alle Ã„nderungen werden in Git getrackt

## ğŸš€ Setup

### 1. Repository erstellen

```bash
# Neues Repository auf GitHub erstellen
# Dann lokal klonen
git clone https://github.com/DEIN-USERNAME/blog-crawler.git
cd blog-crawler
```

### 2. Konfiguration anpassen

Ã–ffne `blog_crawler.py` und passe die BASE_URL an (Zeile ~270):

```python
BASE_URL = "https://www.kerberos-compliance.com/wissen/blog"  # Deine URL hier
```

### 3. Lokaler Test

```bash
# Dependencies installieren
pip install -r requirements.txt

# Crawler manuell ausfÃ¼hren
python blog_crawler.py
```

Die gecrawlten Artikel befinden sich dann im Ordner `blog_content/`.

### 4. Auf GitHub pushen

```bash
git add .
git commit -m "Initial setup"
git push
```

### 5. GitHub Actions aktivieren

Der Workflow lÃ¤uft automatisch:
- **TÃ¤glich um 2:00 Uhr UTC**
- **Bei jedem Push** (fÃ¼r Testing)
- **Manuell Ã¼ber GitHub Actions Tab**

## ğŸ“ Struktur

```
.
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ crawl-blog.yml      # GitHub Actions Workflow
â”œâ”€â”€ blog_content/               # Gecrawlte Artikel (wird erstellt)
â”‚   â”œâ”€â”€ artikel-1.md
â”‚   â”œâ”€â”€ artikel-2.md
â”‚   â”œâ”€â”€ index.json             # Index aller Artikel
â”‚   â””â”€â”€ README.md              # Ãœbersicht der Artikel
â”œâ”€â”€ blog_crawler.py            # Haupt-Crawler-Script
â”œâ”€â”€ requirements.txt           # Python Dependencies
â””â”€â”€ README.md                  # Diese Datei
```

## ğŸ”§ Konfiguration

### Crawling-Zeitplan Ã¤ndern

Bearbeite `.github/workflows/crawl-blog.yml`:

```yaml
schedule:
  # TÃ¤glich um 2:00 Uhr
  - cron: '0 2 * * *'
  
  # Jede Stunde
  # - cron: '0 * * * *'
  
  # Jeden Montag um 8:00 Uhr
  # - cron: '0 8 * * 1'
```

### Output-Verzeichnis Ã¤ndern

In `blog_crawler.py`:

```python
OUTPUT_DIR = "blog_content"  # Anpassen
```

## ğŸ“– Nutzung mit Claude AI

### Option 1: Einzelne Dateien hochladen

1. Gehe zu `blog_content/` in deinem Repository
2. Lade die benÃ¶tigten `.md` Dateien herunter
3. Uploade sie zu Claude mit deiner Anfrage

### Option 2: Gesamtes Archiv nutzen

```bash
# Alle Artikel in eine Datei kombinieren
cd blog_content
cat *.md > alle_artikel.md
```

Dann `alle_artikel.md` zu Claude hochladen.

### Option 3: GitHub Raw Links

Du kannst Claude direkt auf die Raw-Dateien verweisen:

```
https://raw.githubusercontent.com/DEIN-USERNAME/blog-crawler/main/blog_content/artikel-name.md
```

## ğŸ” Was wird gecrawlt?

FÃ¼r jeden Artikel wird extrahiert:

- **Titel**
- **Autor**
- **Datum**
- **Excerpt/Zusammenfassung**
- **VollstÃ¤ndiger Content**
- **URL zum Original**
- **Crawling-Zeitstempel**

## ğŸ“Š Monitoring

### Status Ã¼berprÃ¼fen

1. Gehe zu deinem Repository auf GitHub
2. Klicke auf "Actions"
3. Sieh dir die Workflow-Runs an

### Manuell triggern

1. Gehe zu "Actions" Tab
2. WÃ¤hle "Crawl Blog Content"
3. Klicke "Run workflow"

### Fehler beheben

Logs findest du unter:
- GitHub Actions â†’ Workflow Run â†’ Job Details

## ğŸ› ï¸ Erweiterte Features

### Nur neue Artikel crawlen

StandardmÃ¤ÃŸig werden alle Artikel neu gecrawlt. Um dies zu optimieren, kannst du in `blog_crawler.py` eine PrÃ¼fung einbauen:

```python
def should_crawl(self, url, filepath):
    """PrÃ¼fe ob Artikel neu gecrawlt werden muss"""
    if filepath.exists():
        # PrÃ¼fe Alter der Datei
        age_days = (datetime.now() - datetime.fromtimestamp(filepath.stat().st_mtime)).days
        if age_days < 7:  # Nicht Ã¤lter als 7 Tage
            return False
    return True
```

### Benachrichtigungen einrichten

FÃ¼ge zu `.github/workflows/crawl-blog.yml` hinzu:

```yaml
    - name: Send notification
      if: success()
      uses: someaction/notification@v1
      with:
        message: "Blog crawling completed: ${{ steps.git-check.outputs.changed }}"
```

## ğŸ“ Best Practices fÃ¼r Claude

### KontextgrÃ¶ÃŸe beachten

Claude hat ein Kontextfenster. Bei vielen Artikeln:

1. **Nutze den Index:** `blog_content/index.json` zeigt alle Artikel
2. **Filtere nach Thema:** Lade nur relevante Artikel hoch
3. **Nutze Excerpts:** FÃ¼r Ãœberblick erst die Zusammenfassungen

### Prompt-Beispiele

```
Ich habe dir alle Blog-Artikel aus blog_content/ zur VerfÃ¼gung gestellt. 
Beantworte folgende Frage basierend auf diesen Artikeln: [FRAGE]
```

```
Durchsuche die Blog-Artikel nach Informationen zu [THEMA] und erstelle 
eine Zusammenfassung der wichtigsten Punkte.
```

```
Vergleiche die verschiedenen Artikel zum Thema [THEMA] und zeige mir 
die Entwicklung der Meinungen/Informationen Ã¼ber die Zeit.
```

## ğŸ”’ Sicherheit

- **Keine sensiblen Daten:** Der Crawler speichert nur Ã¶ffentliche Blog-Inhalte
- **Rate Limiting:** 1 Sekunde Pause zwischen Requests
- **Robots.txt:** Respektiert Squarespace Richtlinien

## ğŸ› Troubleshooting

### "No blog posts found"

- ÃœberprÃ¼fe die BASE_URL
- Stelle sicher, dass die HTML-Struktur von Squarespace sich nicht geÃ¤ndert hat
- Teste den Crawler lokal mit `python blog_crawler.py`

### GitHub Actions schlÃ¤gt fehl

- PrÃ¼fe die Logs unter Actions Tab
- Stelle sicher, dass `requirements.txt` korrekt ist
- PrÃ¼fe Python-Version in Workflow (aktuell 3.11)

### Artikel-Content ist leer

Die HTML-Struktur von Squarespace kÃ¶nnte sich geÃ¤ndert haben. Passe die Selektoren in `extract_blog_content()` an.

## ğŸ“„ Lizenz

MIT License - Nutze dieses Tool frei fÃ¼r deine eigenen Blogs.

## ğŸ¤ Contributing

VerbesserungsvorschlÃ¤ge? Erstelle ein Issue oder Pull Request!

## ğŸ“ Support

Bei Fragen oder Problemen Ã¶ffne ein Issue in diesem Repository.

---

**Made with â¤ï¸ for efficient knowledge management with Claude AI**
